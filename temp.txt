Got it âœ… letâ€™s do this step-by-step and cleanly:

---

### ðŸŽ¯ Your setup

1. You already have:

   * A pretrained XLM-R tokenizer (`AutoTokenizer.from_pretrained(model_name)`).
   * A text (corpus) you want to tokenize â†’ so you can extract tokens actually used.
   * A separate **list of token IDs** you want to keep (your reduced vocab).

2. You want:

   * A **new tokenizer** with only those tokens.
   * A **resized model** to match the reduced tokenizer.

---

### âœ… Code

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import sentencepiece as spm
import os

# ----------------------
# 1. Load tokenizer & model
# ----------------------
model_name = "xlm-roberta-base"   # replace with your fine-tuned model if needed
old_tokenizer = AutoTokenizer.from_pretrained(model_name)
old_model = AutoModelForSequenceClassification.from_pretrained(model_name)

# ----------------------
# 2. Collect tokens you want to keep
# ----------------------
# Example: from text
text = "Stock market finance is going up."
tokens = old_tokenizer.tokenize(text)

# Example: from token ids list
token_ids_to_keep = [0, 1, 2, 3, 100, 101]  # replace with your IDs
tokens_from_ids = [old_tokenizer.convert_ids_to_tokens(i) for i in token_ids_to_keep]

# Union of both sets
tokens_to_keep = set(tokens) | set(tokens_from_ids)

# Always keep special tokens
special_tokens = [old_tokenizer.cls_token, old_tokenizer.sep_token,
                  old_tokenizer.pad_token, old_tokenizer.unk_token]
tokens_to_keep.update(special_tokens)

print(f"Keeping {len(tokens_to_keep)} tokens")

# ----------------------
# 3. Train a new SentencePiece model with reduced vocab
# ----------------------
with open("reduced_vocab.txt", "w", encoding="utf-8") as f:
    for tok in tokens_to_keep:
        f.write(tok + "\n")

spm.SentencePieceTrainer.Train(
    f"--input=reduced_vocab.txt "
    f"--model_prefix=reduced_tokenizer "
    f"--vocab_size={len(tokens_to_keep)} "
    "--character_coverage=1.0 "
    "--model_type=unigram"
)

# ----------------------
# 4. Load new tokenizer
# ----------------------
new_tokenizer = AutoTokenizer.from_pretrained(
    "./",              # current dir
    vocab_file="reduced_tokenizer.model",
    use_fast=False
)

print("New tokenizer size:", len(new_tokenizer))

# ----------------------
# 5. Resize model embeddings
# ----------------------
old_vocab_size = old_model.get_input_embeddings().weight.size(0)
new_vocab_size = len(new_tokenizer)

print(f"Resizing embeddings: {old_vocab_size} -> {new_vocab_size}")
old_model.resize_token_embeddings(new_vocab_size)

# ----------------------
# 6. Save reduced tokenizer + model
# ----------------------
save_dir = "reduced_model"
os.makedirs(save_dir, exist_ok=True)
new_tokenizer.save_pretrained(save_dir)
old_model.save_pretrained(save_dir)

print("âœ… Reduced tokenizer + model saved at:", save_dir)
```

---

### ðŸ”‘ What this does

1. Extracts the tokens you want (`tokens_to_keep`) â†’ from your text + from your token ID list.
2. Trains a **new SentencePiece model** (`reduced_tokenizer.model`) only with those tokens.
3. Builds a **new Hugging Face tokenizer** from that `.model`.
4. Resizes your **XLM-R model embeddings** to the smaller vocab.
5. Saves both for Android/TFLite export.

---

âš¡ Now you have a self-contained `reduced_model/` folder â†’
with `config.json`, `pytorch_model.bin`, and `reduced_tokenizer.model` â†’ you can load it like a normal HF model.

---

Do you want me to also show you how to **verify** the new tokenizer really only uses your reduced vocab (like printing its vocab file)?
